{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_Xray_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batyark/chest-x-ray/blob/main/project_Xray_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "שימוש בסיפריות המתאימות"
      ],
      "metadata": {
        "id": "eCSgmUuID33y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dropout\n",
        "import cv2\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l2\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "\n",
        "\n",
        "# importing libraries\n",
        "import glob\n",
        "%matplotlib inline\n",
        "\n",
        "#bokeh\n",
        "from bokeh.models import ColumnDataSource, HoverTool, Panel, FactorRange\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.io import output_notebook, show, output_file\n",
        "from bokeh.palettes import Spectral6"
      ],
      "metadata": {
        "id": "DaRfUO9RD4Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the files via the google drive"
      ],
      "metadata": {
        "id": "zxysOjiYD-6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/chest_xray.zip"
      ],
      "metadata": {
        "id": "mUkQDuO6D85G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c11m5gUpECR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "base_dir = \"/content/chest_xray/\"\n",
        "train_dir = os.path.join(base_dir, \"train/\")\n",
        "test_dir = os.path.join(base_dir, \"test/\")\n",
        "val_dir = os.path.join(base_dir, \"val/\")\n",
        "print(\"Number of images in Train is {}\".format(len(glob(train_dir + \"*/*\"))))\n",
        "print(\"Number of images in Test is {}\".format(len(glob(test_dir + \"*/*\"))))\n",
        "print(\"Number of images in Validation is {}\".format(len(glob(val_dir + \"*/*\"))))\n",
        "train_dir_NORMAL = os.path.join(base_dir, \"train/NORMAL\")\n",
        "print(\"Number of images in Train NORMAL is {}\".format(len(glob(train_dir_NORMAL + \"*/*\"))))\n",
        "train_dir_PNEUMONIA = os.path.join(base_dir, \"train/PNEUMONIA\")\n",
        "print(\"Number of images in Train PNEUMONIA is {}\".format(len(glob(train_dir_PNEUMONIA + \"*/*\"))))\n",
        "test_dir_NORMAL = os.path.join(base_dir, \"test/NORMAL\")\n",
        "print(\"Number of images in test NORMAL is {}\".format(len(glob(test_dir_NORMAL + \"*/*\"))))\n",
        "test_dir_PNEUMONIA = os.path.join(base_dir, \"test/PNEUMONIA\")\n",
        "print(\"Number of images in test PNEUMONIA is {}\".format(len(glob(test_dir_PNEUMONIA + \"*/*\"))))\n",
        "val_dir_NORMAL = os.path.join(base_dir, \"val/NORMAL\")\n",
        "print(\"Number of images in val NORMAL is {}\".format(len(glob(val_dir_NORMAL + \"*/*\"))))\n",
        "val_dir_PNEUMONIA = os.path.join(base_dir, \"val/PNEUMONIA\")\n",
        "print(\"Number of images in val PNEUMONIA is {}\".format(len(glob(val_dir_PNEUMONIA + \"*/*\"))))"
      ],
      "metadata": {
        "id": "2OKL8J12EGNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "data = [[1351, 234, 8],\n",
        "[3875, 390, 8]]\n",
        "X = np.arange(3)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(X + 0.00, data[0], color = 'b', width = 0.25)\n",
        "ax.bar(X + 0.25, data[1], color = 'r', width = 0.25)\n",
        "ax.set_ylabel('Number of images')\n",
        "ax.legend(labels=['NORMAL', 'PNEUMONIA'])"
      ],
      "metadata": {
        "id": "78PLRIjbK3z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "main_path = \"../input/labeled-chest-xray-images/chest_xray\"\n",
        "\n",
        "train_dir = os.path.join(base_dir, \"train/\")\n",
        "test_dir = os.path.join(base_dir, \"test/\")\n",
        "\n",
        "\n",
        "train_normal = glob.glob(train_dir+\"/NORMAL/*.jpeg\")\n",
        "train_pneumonia = glob.glob(train_dir+\"/PNEUMONIA/*.jpeg\")\n",
        "\n",
        "test_normal = glob.glob(test_dir+\"/NORMAL/*.jpeg\")\n",
        "test_pneumonia = glob.glob(test_dir+\"/PNEUMONIA/*.jpeg\")\n",
        "IMG_SIZE = 224\n",
        "BATCH = 16\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "CSXYsvfu8SEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = [x for x in train_normal]\n",
        "train_list.extend([x for x in train_pneumonia])\n",
        "\n",
        "df_train = pd.DataFrame(np.concatenate([['Normal']*len(train_normal) , ['Pneumonia']*len(train_pneumonia)]), columns = ['class'])\n",
        "df_train['image'] = [x for x in train_list]\n",
        "\n",
        "test_list = [x for x in test_normal]\n",
        "test_list.extend([x for x in test_pneumonia])\n",
        "\n",
        "df_test = pd.DataFrame(np.concatenate([['Normal']*len(test_normal) , ['Pneumonia']*len(test_pneumonia)]), columns = ['class'])\n",
        "df_test['image'] = [x for x in test_list]"
      ],
      "metadata": {
        "id": "VujHFwhQ-vXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "חלוקה לוולידציה"
      ],
      "metadata": {
        "id": "U0U1GZ61_N2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(df_train, test_size = 0.15, random_state = SEED, stratify = df_train['class'])\n"
      ],
      "metadata": {
        "id": "lehZn7HX-zK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occur = train_df.groupby(['class']).size()\n",
        "display(occur)\n",
        "COUNT_PNEUMONIA=3293\n",
        "COUNT_NORMAL=1140\n",
        "\n",
        "initial_bias = np.log([COUNT_PNEUMONIA / COUNT_NORMAL])\n",
        "print(\"Initial bias: {:.5f}\".format(initial_bias[0]))\n",
        "\n",
        "TRAIN_IMG_COUNT = COUNT_NORMAL + COUNT_PNEUMONIA\n",
        "weight_for_0 = (1 / COUNT_NORMAL) * (TRAIN_IMG_COUNT) / 2.0\n",
        "weight_for_1 = (1 / COUNT_PNEUMONIA) * (TRAIN_IMG_COUNT) / 2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print(\"Weight for class 0: {:.2f}\".format(weight_for_0))\n",
        "print(\"Weight for class 1: {:.2f}\".format(weight_for_1))\n"
      ],
      "metadata": {
        "id": "1R_hkov6RPJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\", save_best_only=True)\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=10, restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "JKpUx895Tkob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "val_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "ds_train = train_datagen.flow_from_dataframe(train_df,\n",
        "                                             #directory=train_path, #dataframe contains the full paths\n",
        "                                             x_col = 'image',\n",
        "                                             y_col = 'class',\n",
        "                                             target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                             class_mode = 'binary',\n",
        "                                             batch_size = BATCH,\n",
        "                                             seed = SEED)\n",
        "\n",
        "ds_val = val_datagen.flow_from_dataframe(val_df,\n",
        "                                            #directory=train_path,\n",
        "                                            x_col = 'image',\n",
        "                                            y_col = 'class',\n",
        "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                            class_mode = 'binary',\n",
        "                                            batch_size = BATCH,\n",
        "                                            seed = SEED)\n",
        "\n",
        "ds_test = val_datagen.flow_from_dataframe(df_test,\n",
        "                                            #directory=test_path,\n",
        "                                            x_col = 'image',\n",
        "                                            y_col = 'class',\n",
        "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                            class_mode = 'binary',\n",
        "                                            batch_size = 1,\n",
        "                                            shuffle = False)"
      ],
      "metadata": {
        "id": "1YobZ1kV_QKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "קריאה לסיפריות נוספות"
      ],
      "metadata": {
        "id": "Z1mE1mSWAZaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten,Dense,BatchNormalization,Dropout,LeakyReLU,GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from keras import Sequential"
      ],
      "metadata": {
        "id": "xeb5tmr1ADqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "שימוש בRESENET50"
      ],
      "metadata": {
        "id": "viZHmRDAAgTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = ResNet50V2(weights=\"imagenet\",input_shape=(224,224,3),include_top=True)\n",
        "for layer in resnet.layers[:191]:\n",
        "    layer.trainable = False\n",
        "for layer in resnet.layers[191:]:\n",
        "    layer.trainable = True\n",
        "model = Sequential(name=\"my_sequential\")\n",
        "model.add(resnet)\n",
        "model.add(Flatten(input_shape=[224,224,1]))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\n",
        "optim = Adam(lr=1e-3)\n",
        "model.compile(optimizer=optim,loss='binary_crossentropy',metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "M0ybUjRDAjPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "densenet שימוש *ב*"
      ],
      "metadata": {
        "id": "fr3oZEyubZuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "densenet =tf.keras.applications.densenet.DenseNet121(\n",
        "    include_top=True, weights='imagenet', input_tensor=None,\n",
        "    input_shape=(224,224,3), pooling=None, classes=1000\n",
        ")\n",
        "for layer in densenet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model = Sequential(name=\"my_sequential\")\n",
        "\n",
        "model.add(densenet)\n",
        "model.add(Flatten(input_shape=[224,224,1]))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\n",
        "optim = Adam(lr = 1e-2)\n",
        "model.compile(optimizer=optim,loss='binary_crossentropy',metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ipzJQt4Jbgvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "אימון המודל"
      ],
      "metadata": {
        "id": "ErEeUUU0BW_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(ds_train,batch_size = 16, epochs = 100,validation_data=ds_val,steps_per_epoch=16,validation_steps=16,class_weight=class_weight,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb])\n"
      ],
      "metadata": {
        "id": "yBff78S0BZqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy']) \n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy') \n",
        "plt.ylabel('accuracy') \n",
        "plt.xlabel('epoch') \n",
        "plt.legend(['train', 'validation'], loc='lower right') \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n1WPXOt-FvOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss') \n",
        "plt.ylabel('loss') \n",
        "plt.xlabel('epoch') \n",
        "plt.legend(['train', 'validation'], loc='upper right') \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QGI1oN4ChRdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "def predict_classes(predictions):\n",
        "    valResult = predictions.copy()\n",
        "    valResult[valResult <= 0.5] = 0  \n",
        "    valResult[valResult > 0.5] = 1   \n",
        "    return valResult\n",
        "def plot_confusion_matrix(predictions, labels):\n",
        "    valResult = predict_classes(predictions)\n",
        "    conf_matrix_df= pd.DataFrame(confusion_matrix(labels,valResult),\n",
        "                                 index=[\"Normal\", \"Pneumonia\"],\n",
        "                                 columns=[\"Normal\", \"Pneumonia\"])\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    sns.heatmap(conf_matrix_df, \n",
        "                annot=True, \n",
        "                annot_kws={\"size\" : \"20\"}, \n",
        "                fmt='.4g')\n",
        "    plt.xlabel('\\nPredicted', \n",
        "               fontsize=12)\n",
        "    plt.ylabel('Actual', \n",
        "               fontsize=12, \n",
        "               rotation='horizontal', \n",
        "               labelpad=40)\n",
        "    return cm"
      ],
      "metadata": {
        "id": "OX3q6FKHixEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(ds_test)\n",
        "actual_class = ds_test.classes\n",
        "# Confusion Matrix\n",
        "valResult = predict_classes(prediction)\n",
        "cm = confusion_matrix(actual_class, valResult) \n",
        "print('='*50)                                \n",
        "print('True Positive  (TP) = ', cm[0][0])\n",
        "print('False Positive (FP) = ', cm[0][1])\n",
        "print('False Negative (FN) = ', cm[1][0])\n",
        "print('True Negative  (TN) = ', cm[1][1])\n",
        "print('-'*26)\n",
        "plot_confusion_matrix(prediction, actual_class)\n",
        "\n",
        "if (cm[0][0] + cm[0][1])!=0 and (cm[0][0], cm[1][0])!=0:\n",
        "      precision= round((cm[0][0] / (cm[0][0] + cm[0][1]))*100, 2) \n",
        "      recall   = round((cm[0][0] / (cm[0][0] + cm[1][0]))*100, 2)     \n",
        "      print(f'Precision Score: {precision}%')\n",
        "      print(f'Recall Score: {recall}%')\n",
        "      print('='*50) \n",
        "      print('\\n')"
      ],
      "metadata": {
        "id": "SgLUkmLXi0E4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}